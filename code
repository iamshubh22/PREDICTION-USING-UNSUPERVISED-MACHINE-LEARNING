DATA SCIENCE AND BUSINESS ANALYTICS INTERN
TASK 2 :- PREDICTION USING UNSUPERVISED MACHINE LEARNING
BY :- SHUBHKANT DWIVEDI
WHAT IS UNSUPERVISED MACHINE LEARNING ? THE ANSWER IS -> Unsupervised learning is a type of machine learning in which models are trained using unlabeled dataset and are allowed to act on that data without any supervision.Unsupervised learning algorithm will perform this task by clustering the image dataset into the groups according to similarities between images.
TASK DETAILS ARE
THIS IS A SIMPLE K MEANS CLUSTERING TASK THAT IS A PART OF UNSUPERVISED MACHINE LEARNING WHERE WE ARE GOING TO PREDICT THE OPTIMUM NO. OF CLUSTERS AND GOING TO REPRESENT THEM VIRTUALLY FOR THE GIVEN DATASET.
IMPORTING LIBRARIES THAT WILL BE REQUIRED IN THIS TASK FOR THE PREDICTION
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
%matplotlib inline
READING DATASET FROM THE PROVIDED LINK
data_taken=pd.read_csv("Downloads\\Iris.csv")
print("THE DATA HAS BEEN TAKEN SUCCESSFULLY \n")
print(data_taken)
print("\n")
print("THE TOP 5 ELEMENTS ARE")
data_taken.head(5)
THE DATA HAS BEEN TAKEN SUCCESSFULLY 

      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \
0      1            5.1           3.5            1.4           0.2   
1      2            4.9           3.0            1.4           0.2   
2      3            4.7           3.2            1.3           0.2   
3      4            4.6           3.1            1.5           0.2   
4      5            5.0           3.6            1.4           0.2   
..   ...            ...           ...            ...           ...   
145  146            6.7           3.0            5.2           2.3   
146  147            6.3           2.5            5.0           1.9   
147  148            6.5           3.0            5.2           2.0   
148  149            6.2           3.4            5.4           2.3   
149  150            5.9           3.0            5.1           1.8   

            Species  
0       Iris-setosa  
1       Iris-setosa  
2       Iris-setosa  
3       Iris-setosa  
4       Iris-setosa  
..              ...  
145  Iris-virginica  
146  Iris-virginica  
147  Iris-virginica  
148  Iris-virginica  
149  Iris-virginica  

[150 rows x 6 columns]


THE TOP 5 ELEMENTS ARE
Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm	Species
0	1	5.1	3.5	1.4	0.2	Iris-setosa
1	2	4.9	3.0	1.4	0.2	Iris-setosa
2	3	4.7	3.2	1.3	0.2	Iris-setosa
3	4	4.6	3.1	1.5	0.2	Iris-setosa
4	5	5.0	3.6	1.4	0.2	Iris-setosa
TAKING AN INFO OF THE DATASET ALONG WITH A DESCRIBED VIEW
print("INFO FOR THE DATASET IS\n")
data_taken.info()
print("\n")
print("THE DESCRIBED VIEW OF DATA IS\n")
data_taken.describe()
INFO FOR THE DATASET IS

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 150 entries, 0 to 149
Data columns (total 6 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   Id             150 non-null    int64  
 1   SepalLengthCm  150 non-null    float64
 2   SepalWidthCm   150 non-null    float64
 3   PetalLengthCm  150 non-null    float64
 4   PetalWidthCm   150 non-null    float64
 5   Species        150 non-null    object 
dtypes: float64(4), int64(1), object(1)
memory usage: 7.2+ KB


THE DESCRIBED VIEW OF DATA IS

Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm
count	150.000000	150.000000	150.000000	150.000000	150.000000
mean	75.500000	5.843333	3.054000	3.758667	1.198667
std	43.445368	0.828066	0.433594	1.764420	0.763161
min	1.000000	4.300000	2.000000	1.000000	0.100000
25%	38.250000	5.100000	2.800000	1.600000	0.300000
50%	75.500000	5.800000	3.000000	4.350000	1.300000
75%	112.750000	6.400000	3.300000	5.100000	1.800000
max	150.000000	7.900000	4.400000	6.900000	2.500000
DIVIDING DATA INTO THE PARTICULAR DEPENDENT AND INDEPENDENT FEATURES
x=data_taken.iloc[:,[1,4]].values
FINDING THE OPTIMAL NUMBER OF CLUSTERS FOR THE DATASET
from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', 
                    max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') 
plt.show()

TRAINING MODEL USING THE DATASET
kmeans=KMeans(n_clusters=3,init='k-means++',random_state=0)
y_means=kmeans.fit_predict(x)
print(y_means)
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 2 1 1 0 2 1 0 1 1 1 1 2 1 1 1 1 1 1 1 1
 1 2 2 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 0 1 1 1 1 0 1 2 1 2 2 2 2 1 2 2 2 2
 2 2 1 1 2 2 2 2 1 2 1 2 2 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 2 2
 2 1]
REPRESENTING THE OPTIMAL CLUSTERS VIRTUALLY
plt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], 
            s = 100, c = 'blue', label = 'Iris-setosa')
plt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], 
            s = 100, c = 'green', label = 'Iris-versicolour')
plt.scatter(x[y_means == 2, 0], x[y_means == 2, 1],
            s = 100, c = 'red', label = 'Iris-virginica')

# Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], 
            s = 100, c = 'yellow', label = 'Centroids')

plt.legend()
<matplotlib.legend.Legend at 0x19b4097ea60>
